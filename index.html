<!DOCTYPE html>
<html lang="en">

<head>
    <title>Nhan Phan</title>
    <meta charset="utf-8">
    <meta name="description" content="Nhan Phan">
    <meta name="author" content="Nhan Phan">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="refresh" content="600">
    <link rel="stylesheet" type="text/css" href="style.css">
    <link rel="stylesheet" type="text/css" href="style_small.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css"
        integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.7.1/jquery.min.js"></script>
    <script src="//toolness.github.io/p5.js-widget/p5-widget.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.7.0/p5.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.7.0/addons/p5.sound.min.js"></script>


</head>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-ZPJHJ1TMV0"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-ZPJHJ1TMV0');
</script>

<body>
    <div class="back-to-left">
        <button onclick="topFunction()" id="myBtn" title="Go to top">Top</button>
    </div>

    <div class="page-container">
        <div class="container-offset">
            <section id="bio">
                <div class="nav-header">
                    <h1>Nhân Phan</h1>
                </div>
                <div style="text-align:justify">
                    <p>
                        Nhân Phan is a technologist and an educator based in Ho Chi Minh City, Vietnam.
                        His practice explores coding & images as a shelter for his memory, dream, & fantasy.
                    </p>
                    <!-- <p>
                        Born to the era of Yahoo! 360 Blog, Flickr, and Tumblr, I grew up making blogs, scrapbooks, and
                        CD covers from photos I took and from the net. To me, photos are beautiful, sentimental, and
                        nostalgic. The web has changed ever since. The new medium of coding allows me to look back into my
                        archival practice in a different way but that wistful feeling shall never fade.
                    </p> -->

                    <p>
                        IG @ <a href="https://instagram.com/nhaninsummer">nhaninsummer</a>
                        <br>Gmail @ <a href="mailto:nhaninsummer@gmail.com">nhaninsummer</a>.
                    </p>
                </div>
            </section>

            <section id="navigation">
                <div class="nav">
                    <div class="nav-content-block">
                        <h2 class="nav-sub-header">Teach
                        </h2>
                        <div class="nav-content pl-4 pb-3">
                            <p class="mb-1">In 2023, I founded a study club called <a
                                    href="https://instagram.com/codesurfing.club"
                                    target="_blank"><b>"CodeSurfing".</b></a></p>
                            <p class="mb-1">From 2020-2022, I taught <a href="#" target="_blank">Data and Computer
                                    Vision</a>.</p>
                        </div>
                    </div>

                    <div class="nav-content-block">
                        <h2 class="nav-sub-header">Selected Works</h2>
                        <div class="nav-content pl-4 pb-3">
                            <p class="mb-1"><a href="#wa">(2024) 和</a></p>
                            <p class="mb-1"><a href="#ocean">(2023) “i let the fish into the ocean.”</a></p>
                            <p class="mb-1"><a href="#portrait">(2023) portrait</a></p>
                            <p class="mb-1"><a href="#beach-pocket">(2022) beach pocket</a></p>
                            <p class="mb-1"><a href="#gai-gia">(2022) ガイジャ別府</a></p>
                            <p class="mb-1"><a href="#live-laugh-dick">(2022) live. laugh. dick(s).</a></p>
                            <p class="mb-1"><a href="#wes-anderson">(2021) wes anderson</a></p>
                            <p class="mb-1"><a href="#esrgan">(2020) esrgan</a></p>
                            <p class="mb-1"><a href="#viet-ocr">(2020) viet ocr</a></p>
                            <p class="mb-1"><a href="#amazon">(2019) amazon</a></p>
                        </div>
                    </div>

                    <!-- <div class="nav-content-block">
                        <h2 class="nav-sub-header"><a href="before2020.html"
                                style="text-decoration:none!important; color:rgb(148, 58, 233)">Before 2020 </a></h2>
                    </div> -->

                </div>
            </section>

            <section id="wa">
                <h2>
                    (2024) 和
                </h2>
                <div class="tag-container">
                    <span class='tag'>p5.js</span>
                </div>
                <a data-fslightbox="gallery" href="assets/wa/1.png"><img src="assets/wa/1.png"></a>
                <a data-fslightbox="gallery" href="assets/wa/2.png"><img src="assets/wa/2.png"></a>
            </section>

            <section id="ocean">
                <h2>
                    (2023) “i let the fish into the ocean.”
                </h2>
                <div class="tag-container">
                    <span class='tag'>p5.js</span>
                </div>
                <div>
                    <img src="assets/jump/jump_0.png">
                    <p>
                        From 2023 footages in my phone, “i let the fish into the ocean” is a
                        meditative montage on my intimacy with nature ⎯ water, air, and biological bodies.
                        ⎯ through the way we flow, intertwine, and harmonize this school of lives together.
                    </p>
                    <p><a href="https://symphonious-kleicha-cbc328.netlify.app/" target="_blank">Leave this page for
                            that
                            realm.</a></p>

                    <br>
                    <p style="text-align: right;"><i>Background image: seoulsoloist</i></p>

                    <!-- <embed src="https://symphonious-kleicha-cbc328.netlify.app/"> -->
                </div>
            </section>

            <section id="portrait">
                <h2>
                    (2023) portrait
                </h2>
                <div class="tag-container">
                    <span class='tag'>p5.js</span>
                </div>
                <div>
                    <div id="portrait-display"></div>
                    <p style="text-align: right; color:rgba(40, 40, 40, 0.847);"><i>poem by<br>Ocean Vương</i></p>
                    <p>
                        <br>the year is 2017,
                        <br>scorching japan's summer
                        <br>i'm with him in the shower
                        <br>sun, everywhere.
                    </p>

                </div>
            </section>

            <section id="beach-pocket">
                <h2>
                    (2022) Beach Pocket
                </h2>
                <div class="tag-container">
                    <span class='tag'>photo</span>
                    <span class='tag'>print</span>
                </div>
                <div>
                    <br>
                    <a data-fslightbox="gallery" href="assets/beach-pocket/4.png"><img src="assets/beach-pocket/4.png" style="width:60%"></a>
                    <br>
                    <p>
                        First thing I said when a guy took my shirt off was always ⎯ “Am I too skinny?”
                    </p>
                    <p>
                        From 2019 to 2022, my best friends and I made occasional getaways to the beaches in Vietnam.
                        There we swam, sunbathed, read, danced, and radiated under the sun. There I found comfort in my
                        own skin. There I realized the beauty of our shapes. The whole process is a healing journey for
                        me.
                    </p>
                    <p>
                        This pocket notebook includes all the portraits and self-portraits that I documented during the
                        time.
                    </p>

                    <a data-fslightbox="gallery" href="assets/beach-pocket/1.png"><img class="beach-pocket-img" src="assets/beach-pocket/1.png"></a>
                    <a data-fslightbox="gallery" href="assets/beach-pocket/5.png"><img class="beach-pocket-img" src="assets/beach-pocket/5.png"></a>
                    <a data-fslightbox="gallery" href="assets/beach-pocket/2.png"><img class="beach-pocket-img" src="assets/beach-pocket/2.png"></a>
                    <a data-fslightbox="gallery" href="assets/beach-pocket/6.png"><img class="beach-pocket-img" src="assets/beach-pocket/6.png"></a>
                    <a data-fslightbox="gallery" href="assets/beach-pocket/7.png"><img class="beach-pocket-img" src="assets/beach-pocket/7.png"></a>
                    <a data-fslightbox="gallery" href="assets/beach-pocket/8.png"><img class="beach-pocket-img" src="assets/beach-pocket/8.png"></a>
                    <a data-fslightbox="gallery" href="assets/beach-pocket/3.png"><img class="beach-pocket-img" src="assets/beach-pocket/3.png"></a>

                    <p>
                        <br>“My boyfriend once said that I was so tiny
                        <br>That he could carry me in his pocket anywhere
                        <br>So put me in your pocket
                        <br>Use me as your time goes by
                        <br>Use my body as your late-night canvas

                        <br>Write on me
                        <br>Compose on me
                        <br>Fast on me
                        <br>Slow on me
                        <br>Release on me
                        <br>Spit on me
                        <br>Piss on me
                        <br>Bleed on me”


                        <br><br>✣＊✣

                        <br>Produced by <a href="https://wedogood.party" target="_blank" class="link-out">wedogood</a>.
                        <br>64 pages on risograph using aqua, yellow, flourescent pink.
                    </p>
                </div>
            </section>

            <section id="gai-gia">
                <h2>
                    (2022) ガイジャ別府
                </h2>
                <div class="tag-container">
                    <span class='tag'>photo</span>
                    <span class='tag'>print</span>
                </div>
                <div>
                    <img style="width:70%" src="assets/gai-gia/gaigiabeppu_zine01 copy.png">
                    
                    <br>
                    <p>
                        Every year when the cicadas start to sing, I miss Japan dearly, as if a part of myself had been
                        buried under the Minami Ishigaki park, where we hung out by the swings, singing, and smoking.
                    </p>

                    <p>
                        This summer, as the cicadas are singing again, I invited Cao Mieu to join me in a conversation
                        about our Japanese memoirs. But instead of texts, we would reply to each other with artworks.
                        Every page is a response to the previous. All communication takes place only within these pages.
                    </p>

                    <p>
                        I lost my residence card years ago. Mieu still has hers, so she will hereby board the page
                        first.
                    </p>


                    <a data-fslightbox="gallery" href="assets/gai-gia/gaigiabeppu_zine01.jpg"><img style="width:70%" src="assets/gai-gia/gaigiabeppu_zine01.jpg"></a>
                    <a data-fslightbox="gallery" href="assets/gai-gia/gaigiabeppu_zine02.jpg"><img style="width:70%" src="assets/gai-gia/gaigiabeppu_zine02.jpg"></a>
                    <a data-fslightbox="gallery" href="assets/gai-gia/gaigiabeppu_zine03.jpg"><img style="width:70%" src="assets/gai-gia/gaigiabeppu_zine03.jpg"></a>
                    <a data-fslightbox="gallery" href="assets/gai-gia/gaigiabeppu_zine04.jpg"><img style="width:70%" src="assets/gai-gia/gaigiabeppu_zine04.jpg"></a>
                    <a data-fslightbox="gallery" href="assets/gai-gia/gaigiabeppu_zine05.jpg"><img style="width:70%" src="assets/gai-gia/gaigiabeppu_zine05.jpg"></a>
                    <a data-fslightbox="gallery" href="assets/gai-gia/gaigiabeppu_zine06.jpg"><img style="width:70%" src="assets/gai-gia/gaigiabeppu_zine06.jpg"></a>
                    <a data-fslightbox="gallery" href="assets/gai-gia/gaigiabeppu_zine07.jpg"><img style="width:70%" src="assets/gai-gia/gaigiabeppu_zine07.jpg"></a>
                    <a data-fslightbox="gallery" href="assets/gai-gia/gaigiabeppu_zine08.jpg"><img style="width:70%" src="assets/gai-gia/gaigiabeppu_zine08.jpg"></a>
                    <a data-fslightbox="gallery" href="assets/gai-gia/gaigiabeppu_zine09.jpeg"><img style="width:70%" src="assets/gai-gia/gaigiabeppu_zine09.jpeg"></a>
                    <a data-fslightbox="gallery" href="assets/gai-gia/gaigiabeppu_zine10.jpg"><img style="width:70%" src="assets/gai-gia/gaigiabeppu_zine10.jpg"></a>
                    <a data-fslightbox="gallery" href="assets/gai-gia/gaigiabeppu_zine11.jpg"><img style="width:70%" src="assets/gai-gia/gaigiabeppu_zine11.jpg"></a>
                    <a data-fslightbox="gallery" href="assets/gai-gia/gaigiabeppu_zine12.jpg"><img style="width:70%" src="assets/gai-gia/gaigiabeppu_zine12.jpg"></a>

                </div>
            </section>

            <section id="live-laugh-dick">
                <h2>
                    (2022) Live. Laugh. Dick(s).
                </h2>

                <div class="tag-container">
                    <span class='tag'>Python</span>
                    <span class='tag'>print</span>
                </div>

                <div>
                    <a data-fslightbox="gallery" href="assets/live-laugh-dicks/1.png"><img src="assets/live-laugh-dicks/1.png"></a>
                    <a data-fslightbox="gallery" href="assets/live-laugh-dicks/2.png"><img src="assets/live-laugh-dicks/2.png"></a>
                    <p>
                        Back in April, wedogood invited me to join their zine with the theme of “Love Machine. Machine
                        Love”. And all I brought was erotism, fantasy, and re-imagination. This poster is a stand-alone
                        version of my work in the zine. More than a collection of quirky-looking toys, It reflects our
                        current perception of sex toy design (dildos and butt plugs in particular) while suggesting new
                        boundaries for toy design.
                    </p>

                    <a data-fslightbox="gallery" href="assets/live-laugh-dicks/3.png"><img src="assets/live-laugh-dicks/3.png"></a>
                    <a data-fslightbox="gallery" href="assets/live-laugh-dicks/4.png"><img src="assets/live-laugh-dicks/4.png"></a>

                    <p>
                        After being trained with 3000 photos of toys, the generative model clearly gets the idea that a
                        sex toy needs to be pointed (of course). But it takes the idea further by re-imagining toys with
                        multiple heads, and toys with irregular shapes or shapes that are different from cylinders.
                        Several generated samples also include toys that are bound together since e-commerce often
                        places their toys next to each other in product photos. If such an arrangement stimulates the
                        buyer, then why not include them in the real product design? Many of the generated samples also
                        propose getting rid of the inside of the toys as it is not a significant feature. They suggest
                        void, disjoint parts, transparent material, and anything else but the common solid shape.

                        Pleasure has its own curiosity. And maybe toys for pleasure should also be more suggestive,
                        rather than adaptive.

                    </p>
                    <p>

                        This project is built on my custom GAN model, inspired by StyleGAN2. The StyleGAN2 architecture
                        itself is gigantic. To afford training, I made multiple adjustments in the architecture,
                        including downsizing the output image size to 128x128. This seriously damaged the print quality
                        but Risograph helped me bypass that. I also divided the training into multiple sessions + used
                        the Tensorflow Data Dataset & Tensorflow Record to optimize the whole training speed. All are
                        for this project to be run on the free resource of Google Colab, which has a limited quota every
                        day. So much engineering just to have more dicks while paying less 🥴

                    </p>


                    <a data-fslightbox="gallery" href="assets/live-laugh-dicks/6.gif"><img src="assets/live-laugh-dicks/6.gif"></a>
                    <a data-fslightbox="gallery" href="assets/live-laugh-dicks/7.png"><img src="assets/live-laugh-dicks/7.png"></a>
                    <a data-fslightbox="gallery" href="assets/live-laugh-dicks/8.gif"><img src="assets/live-laugh-dicks/8.gif"></a>

                    <p>
                        <br>WHY RISOGRAPH?

                        <br><br>Generative art is not for size queen. Artworks generated from ML model struggle to have
                        a good
                        resolution. A simple image of 300x300 would take 90,000 units when being flattened. It means
                        that a larger output images come with a larger cost of computation. It often requires days of
                        training on expensive GPU. When it comes to printing, this limit in output results in pixelating
                        details, blurry edges, and inconsistent separation between object and background. Not only that,
                        generative images oftentimes have the checkerboard effect, as a result that the machine
                        “painted” each pixel independently and lack of perception of the image as a whole.

                    </p>
                    <p>
                        In order to produce this digital artwork in high-quality print (A3), we first put the 128x128
                        generated images through a half-toned treatment - a technique to simulate the image tone through
                        dots. By carefully adjusting the dot size, we gave the pixelated images a sharper optic illusion
                        in general. A subtly similar pair of aqua ink and purple paper were then chosen to let the
                        half-toned dots blend smoothly with the background. The various size of dots + different % of
                        ink embrace the blurry edge. The aqua ink also expands optically when we tilt the poster to
                        different light direction. Object edges “fade” gradually into paper like chalk. The drawback of
                        pixelating and not having sharp edges is now a compliment toward the initial inspiration of
                        stains.
                    </p>

                    <a data-fslightbox="gallery" href="assets/live-laugh-dicks/9.png"><img src="assets/live-laugh-dicks/9.png"></a>
                    <a data-fslightbox="gallery" href="assets/live-laugh-dicks/10.png"><img src="assets/live-laugh-dicks/10.png"></a>

                    <p>
                        Crossing between multiple forms ⎯ from photographs, to numbers, to logic, to a new form of
                        photographs, then comes alive as a print. A journey from modern computation to a long-lived
                        printing technique; from abstract to physical, with which we can see, can touch, and can
                        interact.

                        I think it is beautiful.
                    </p>

                    <a data-fslightbox="gallery" href="assets/live-laugh-dicks/11.png"><img src="assets/live-laugh-dicks/11.png"></a>
                    <a data-fslightbox="gallery" href="assets/live-laugh-dicks/12.png"><img src="assets/live-laugh-dicks/12.png"></a>
                    <p><i>background: IG/manual_singularity</i></p>

                </div>
            </section>

            <section id="wes-anderson">
                <div>
                    <h2>
                        (2021) Watching Wes Anderson Without Watching Wes Anderson
                    </h2>

                    <div class="tag-container">
                        <span class='tag'>Python</span>
                        <span class='tag'>data analysis</span>
                    </div>

                    <a data-fslightbox="gallery" href="assets/wes-anderson/aqua_1.png"><img src="assets/wes-anderson/aqua_1.png"></a>
                    <a data-fslightbox="gallery" href="assets/wes-anderson/dog_1.png"><img src="assets/wes-anderson/dog_1.png"></a>
                    <a data-fslightbox="gallery" href="assets/wes-anderson/hotel_1.png"><img src="assets/wes-anderson/hotel_1.png"></a>
                    <a data-fslightbox="gallery" href="assets/wes-anderson/moon_1.png"><img src="assets/wes-anderson/moon_1.png"></a>

                    <p class="pt-2">
                        In 2021, Saigon went dormant under COVID lockdown. No one can set foot beyond their door. So did
                        I
                        and my housemate. We ended up doing a marathon of Wes Anderson movies. Going through all his
                        movies
                        was like walking on a train, that moving so fast, that all the beautiful scenes become color
                        running
                        across my window. Color is the main actor in his movies.
                    </p>
                    <p>
                        The normal way to watch a movie requires audiences to sit through frame by frame. A movie
                        presents
                        itself linearly with time with visual elements built up on top of each other. This project
                        challenges that concept and aims to understand the visual landscape of Wes Anderson movies
                        through
                        just one single look.
                    </p>
                    <p>
                        To achieve that, each frame of the film was flattened from a rectangular shape (720x1280) to
                        into a
                        long strip (1x921,600). Then, all the strips were stacked on top of each other to create the
                        final
                        artwork. As a result, vertically, from top to bottom, we are “watching” the movie from the
                        beginning
                        to the end. Horizontally, from left to right, our eyes are moving zig-zag in one scene of the
                        movie
                        (left to right, top to bottom).
                    </p>
                    <p>
                        In the end, this project reveals how Wes Anderson uses colors to create the world surrounding
                        his
                        characters, and how that colorful world flows according to his characters' emotion.
                    </p>
                    <h3>
                        1/ The Life Aquatic with Steve Zissou (2004)
                    </h3>

                    <a data-fslightbox="gallery" href="assets/wes-anderson/aqua_1.png"><img src="assets/wes-anderson/aqua_1.png"></a>

                    <p>
                        One of the first Wes Anderson’s, dated back in 2004. The movie's color is everything but its
                        name “aqua”. Most of the scenes are in a warm, earthy tone while the color aqua is used as
                        highlights scattered throughout the movie.</p>

                    <p>When looking closer, I found out that aqua was used specifically as a way to revert the movie's
                        emotion - from the scene where Steve met his wife, to the pirates’ raid, and to his son’s last
                        moment. The color aqua sets the boundary of bright sunshine and dark ocean, surface and below,
                        inward and outward, carefree voyage life and emotional tension. It makes the peak and valley in
                        Steve Zissou’s life.
                    </p>
                    <a data-fslightbox="gallery" href="assets/wes-anderson/aqua_3.png"><img src="assets/wes-anderson/aqua_3.png"></a>
                    <h3>
                        2/ Moonrise Kingdom (2012)
                    </h3>
                    <a data-fslightbox="gallery" href="assets/wes-anderson/moon_1.png"><img src="assets/wes-anderson/moon_1.png"></a>
                    <p>
                        Moonrise Kingdom is divided into two distinctive palettes: before the storm and after the storm.
                        The “before the storm” embraces the warm colors of yellow, green, and brown, with scenes mostly
                        shot in bright sunlight while the “after the storm” rages in cooler shades of blue and teal,
                        with lots of scenes without the sun or even in the dark.
                    </p>
                    <a data-fslightbox="gallery" href="assets/wes-anderson/moon_2.png"><img src="assets/wes-anderson/moon_2.png"></a>
                    <a data-fslightbox="gallery" href="assets/wes-anderson/moon_3.png"><img src="assets/wes-anderson/moon_3.png"></a>
                    <a data-fslightbox="gallery" href="assets/wes-anderson/moon_4.png"><img src="assets/wes-anderson/moon_4.png"></a>
                    <p>
                        The transition from the bright color to the darker one doesn’t follow the change of nature (the
                        arrival of the storm) in the movie. It, however, follows the transition of the characters’
                        emotions. The change started right after Sam and Suzy got caught by the beach. The following
                        scene of Suzy’s conversation with her mom immediately takes the sunlight out and drowns the
                        movie in the cold tub. Perhaps, the movie’s real storm already raged after that conversation.
                    </p>

                    <a data-fslightbox="gallery" href="assets/wes-anderson/moon_6.png"><img src="assets/wes-anderson/moon_6.png"></a>

                    <h3>
                        3/ The Grand Budapest Hotel (2014)
                    </h3>
                    <a data-fslightbox="gallery" href="assets/wes-anderson/hotel_1.png"><img src="assets/wes-anderson/hotel_1.png"></a>

                    <p>
                        The movie has many noticeable black columns that run vertically. Their widths vary in the
                        beginning, then become consistent as the movie goes on. These black columns are created from the
                        black margin of the frames. Different size of black margin signifies different screen ratios. In
                        fact, Wes Anderson intentionally used different screen ratios to mimic different eras’ cinematic
                        styles. The 80’s ⎯ 1.85 : 1, The 60’s ⎯ 2.40 : 1, The 30’s ⎯ 1.37:1.
                    </p>
                    <a data-fslightbox="gallery" href="assets/wes-anderson/hotel_2.png"><img src="assets/wes-anderson/hotel_2.png"></a>
                    <p>
                        The movie is clearly divided into blocks of colors. Each group of scenes is in one distinctive
                        palette of color. The transition of color is both more extreme and playful than in his early
                        works - Moonrise Kingdom and The Life Aquatic of Steve Zissou.
                    </p>
                    <a data-fslightbox="gallery" href="assets/wes-anderson/hotel_3.png"><img src="assets/wes-anderson/hotel_3.png"></a>

                    <h3>
                        4/ Isle of Dogs (2018)
                    </h3>
                    <a data-fslightbox="gallery" href="assets/wes-anderson/dog_1.png"><img src="assets/wes-anderson/dog_1.png"></a>
                    <p>
                        Continued with the idea of using colors to define space for characters’ emotions, Isle of Dogs
                        used extreme colors, black and white, to depict two different groups of scenes: the trash island
                        and the city hall. Yellow strips that run horizontally between them are Tracy Walker. She brings
                        light to the revolution of Atari and the dogs.
                    </p>
                    <a data-fslightbox="gallery" href="assets/wes-anderson/dog_2.png"><img src="assets/wes-anderson/dog_2.png"></a>
                    <p>
                        Several groups of scenes in Isle of Dogs maintain a fixed layout, with both characters and the
                        camera making minimal moves. For example, in the white strip area in the middle, we can see that
                        the black area (the characters) stays in place for several continuous scenes. This can be the
                        effect of stop motion, where continuous scenes have very subtle changes, so the audiences can
                        really focus on such change and the “stop-motion delay” between the change. For example, the
                        making sushi scene.

                        However, when considering other movies by Wes Anderson, The Grand Budapest Hotel also has this
                        same pattern. Many scenes in the movie, especially scenes where characters discuss, have very
                        minimal camera movement. So rather than highlighting the effect of stop motion, in these scenes
                        of Isle of Dog, Wes Anderson is leveraging stop motion to achieve his own distinctive technique.
                        They play out so well and compliment each other.
                    </p>
                    <a data-fslightbox="gallery" href="assets/wes-anderson/dog_3.png"><img src="assets/wes-anderson/dog_3.png"></a>
                    <p>
                        <i>Isle of Dogs</i>
                    </p>
                    <a data-fslightbox="gallery" href="assets/wes-anderson/dog_4.png"><img src="assets/wes-anderson/dog_4.png"></a>
                    <p>
                        <i>The Grand Budapest Hotel</i>
                    </p>

                    <hr>
                    <p><i>Inspiration</i></p>
                    <img src="assets/wes-anderson/raucau.jpeg">
                </div>
            </section>

            <section id="esrgan">
                <h2>
                    (2020) Enhanced Super Resolution GAN on Tensorflow 2
                </h2>
                <div class="tag-container">
                    <span class='tag'>Python</span>
                    <span class='tag'>machine learning</span>
                </div>
                <div>
                    <a data-fslightbox="gallery" href="assets/esrgan/1.jpeg"><img src="assets/esrgan/1.jpeg"></a>
                    <p>
                        VISION2020 aims at recovering a high resolution image from a low resolution one. The project is
                        based largely on the excellent research of Xintao Wang, et al. on ESRGAN (2018) and their
                        implementation using Pytorch. Inspired from the research, my version of ESRGAN is optimized and
                        built entirely on Tensorflow 2.0. It successfully resizes the image up to x64 on square area.
                    </p>
                    <p>
                        Single image super-resolution (SISR), as a fundamental low-level vision problem, has attracted
                        increasing attention in the research community and AI companies. SISR aims at recovering a
                        high-resolution (HR) image from a single low-resolution (LR) one. Since the pioneer work of
                        SRCNN
                        proposed by Dong et al., deep convolution neural network (CNN) approaches have brought
                        prosperous
                        development. Various network architecture designs and training strategies have continuously
                        improved
                        the SR performance.
                    </p>
                    <p>

                        The Super-Resolution Generative Adversarial Network (SRGAN) is a seminal work that is capable of
                        generating realistic textures during single image super-resolution. However, the hallucinated
                        details are often accompanied with unpleasant artifacts. To further enhance the visual quality,
                        we
                        thoroughly study three key components of SRGAN - network architecture, adversarial loss and
                        perceptual loss, and improve each of them to derive an Enhanced SRGAN (ESRGAN).
                    </p>
                    <p>
                        In particular, we introduce the Residual-in-Residual Dense Block (RRDB) without batch
                        normalization
                        as the basic network building unit. Moreover, we borrow the idea from relativistic GAN to let
                        the
                        discriminator predict relative realness instead of the absolute value. Finally, we improve the
                        perceptual loss by using the features before activation, which could provide stronger
                        supervision
                        for brightness consistency and texture recovery. Benefiting from these improvements, the
                        proposed
                        ESRGAN achieves consistently better visual quality with more realistic and natural textures than
                        SRGAN.
                    </p>

                    <a data-fslightbox="gallery" href="assets/esrgan/2.png"><img src="assets/esrgan/2.png"></a>

                    <p>
                        fig1 ⎯ (x4 per dimension) generated image successfully retains small detail like the strip at
                        the shoulder area and the human head.
                    </p>
                    <a data-fslightbox="gallery" href="assets/esrgan/3.png"><img src="assets/esrgan/3.png"></a>
                    <p>
                        fig2 ⎯ (x4 per dimension) Natural features like eyes are well reconstructed.
                    </p>
                    <a data-fslightbox="gallery" href="assets/esrgan/4.png"><img src="assets/esrgan/4.png"></a>
                    <p>
                        fig3 ⎯ (x8 per dimension) Double challenging, then model successfully reconstruct pattern and
                        lines.
                    </p>
                    <a data-fslightbox="gallery" href="assets/esrgan/5.png"><img src="assets/esrgan/5.png"></a>
                    <p>
                        fig4 ⎯ (x8 dimension) Letters are brought back to vision.
                    </p>

                    <p>
                        Full project code is available on <a href="https://github.com/nhanphan0411/VISION2020"
                            class="link-out" target="blank_">Github</a>
                    </p>

                </div>
            </section>

            <section id="viet-ocr">
                <h2>
                    (2020) Vietnamese Handwritten Optical Character Recogition
                </h2>
                <div class="tag-container">
                    <span class='tag'>Python</span>
                    <span class='tag'>machine learning</span>
                </div>
                <div>
                    <p>
                        Optical Character Recognition is one active field that bridges between computer vision and
                        natural language processing. As much as the field emerges within machine learning community, it
                        still performs poorly on local language, including Vietnamese with our distinctive symbol (ễ, ẩ,
                        ứ for example). The lack of data is one of the main reason behind it. In 2018, Cinnamon AI aimed
                        to solve that challange by hosting a hackathon with a Vietnamese handwritten dataset. It
                        includes all the address written in Vietnamese. The model can be immediately apply in post
                        service to alleviate the need of manual input.
                    </p>
                    <p>
                        All code of this project can be found on my <a href="https://github.com/nhanphan0411/viet-ocr"
                            class="link-out" target="blank_">Github 👾</a>
                    </p>

                    <a data-fslightbox="gallery" href="assets/ocr/1.png"><img src="assets/ocr/1.png"></a>

                    <p>
                        ❊ RESULT ❊

                        My project successfully achieved
                        Character Error Rate: 0.04
                        Word Error Rate: 0.14
                        Sentence Error Rate: 0.82
                    </p>
                    <p>
                        The hackathon's winner score is 0.1x on the Word Error Rate.
                        Other metric results were not disclosed.
                    </p>
                    <p>
                        ❊ SAMPLE PREDICTIONS ❊
                        T = True Label
                        P = Prediction
                    </p>
                    <a data-fslightbox="gallery" href="assets/ocr/2.png"><img src="assets/ocr/2.png"></a>
                    <p>
                        ❊ IMAGE PREPROCESS ❊
                        The preprocess was built mainly on OpenCV with 3 phases
                        1/ Thresholding
                        2/ Resize to 128x1024
                        3/ Remove Recursive (reference to A. Vinciarelli and J. Luettin)

                        (Before - After)
                    </p>
                    <a data-fslightbox="gallery" href="assets/ocr/3.png"><img src="assets/ocr/3.png"></a>
                    <a data-fslightbox="gallery" href="assets/ocr/4.png"><img src="assets/ocr/4.png"></a>
                    <p>
                        ❊ MODEL ❊
                        CRNN + CTC Loss is used to solve this challenge.
                        CNN blocks with skip connections (inspired by ResNet50) are used to extract the features from
                        the input image.
                        The extracted feature map will be then passed through the LSTM layers.
                    </p>
                    <a data-fslightbox="gallery" href="assets/ocr/5.png"><img src="assets/ocr/5.png"></a>
                    <br>
                    <p>Training Log</p>
                    <a data-fslightbox="gallery" href="assets/ocr/6.png"><img src="assets/ocr/6.png"></a>
                </div>
            </section>

            <section id="amazon">
                <h2>
                    (2019) Understand The Amazon From Above
                </h2>
                <div class="tag-container">
                    <span class='tag'>Python</span>
                    <span class='tag'>machine learning</span>
                </div>
                <div>
                    <img src="assets/amazon/1.jpg">

                    <p><i>This project is an entry of the corresponding <a
                                href="https://www.kaggle.com/c/planet-understanding-the-amazon-from-space/data"
                                class="link-out">Kaggle competition.</a></i></p>

                    <p>
                        Every minute, the world loses an area of forest the size of 48 football fields. And
                        deforestation in the Amazon Basin accounts for the largest share, contributing to reduced
                        biodiversity, habitat loss, climate change, and other devastating effects. But better data about
                        the location of deforestation and human encroachment on forests can help governments and local
                        stakeholders respond more quickly and effectively.
                    </p>

                    <p>
                        This analysis uses Deep Learning to classify the spatial images of the Amazon forest taken by
                        the satellite. From that, it hopes to shed a light on understanding how the forest has change
                        naturally and manually. Thus, help preventing deforestation.
                    </p>

                    <img src="assets/amazon/2.png">
                    <br>
                    <p>
                        The project is built on dataset from the Kaggle competition in 2016. It contains more than
                        40.000 images, taken by Planet using sattelites.
                    </p>
                    <p>
                        Planet, designer and builder of the world’s largest constellation of Earth-imaging satellites,
                        will soon be collecting daily imagery of the entire land surface of the earth at 3-5 meter
                        resolution. While considerable research has been devoted to tracking changes in forests, it
                        typically depends on coarse-resolution imagery from Landsat (30 meter pixels) or MODIS (250
                        meter pixels). This limits its effectiveness in areas where small-scale deforestation or forest
                        degradation dominate.
                    </p>

                    <h3>Result</h3>
                    <p>
                        The project successfully got the score of 0.90 on the official test set.
                    </p>
                    <img src="assets/amazon/3.png">
                    <img src="assets/amazon/4.png">

                    <h3>Challenge</h3>
                    <p>1️/ Multi-label: Each image is labeled with multiple tags (at least 2, at max 9). The tags fall
                        into
                        17 categories, which are the forest landscape types. Since the tags in each label are mutually
                        exclusive, they are treated as multiple binary classification problems. Thus, binary
                        cross-entropy
                        are chosen to be the loss function.
                    </p>

                    <p>
                        2️/ Imbalance: The dataset is severely imbalance with tags like Primary or Agriculture appear in
                        90%
                        of the dataset. While other tags like Blooming or Conventional Mine can only be seen in less
                        than
                        500 observations (even less than 100 for Burn Down).
                    </p>
                    <p>
                        In the first base-line experiment, the model was totally bias toward the major tags. It predicts
                        the
                        major tags to appear in every data and almost never made a prediction with the minor tags.
                        To tackle the problem of imbalance dataset, evaluation metrics must be chosen carefully. F2 is
                        chosen to be the main metrics to evaluate the training. It watches over the harmonic mean
                        between
                        the Precision and Recall while favors Recall specifically. In other word, it is the attempt to
                        reduce the number of False Negative, where the model fails to identify the absence of a tag.
                    </p>
                    <p>
                        3️/ Optimization: 400.000 images, a CNN model, and Google Colab's limited resource do not seem
                        to
                        mix well together. The training was slow at first and interupted often. Several improvements,
                        mostly
                        on the Tensorflow pipeline, were conducted to speed up the training:
                        Using TFRecord to convert the raw images into byte-like data to reduce the amount of time
                        spending
                        on reading data from their paths.
                    </p>
                    <p>
                        Using tf.data.Dataset with shuffle, map, batch, prefetch to optimize the reading data process by
                        redistributing the tasks for agents to work concurrently, thus, avoid bottleneck. An attempt to
                        use
                        cache was also made but failed due to the limited RAM.
                    </p>
                    <p>
                        Processing image with Tensorflow: The dataset contains images in JPG - RGBA. The built-in decode
                        function tf.io.decode_jpeg only works on 1 or 3-channel image. Attempt on encoding a JPG RGBA
                        image
                        returns black black and black. We need a tensorflow encoding function to work in this part
                        because
                        the pipeline is built entirely on Tensor for the optimization purpose.
                        To tackle the problem, the raw images were first read by Matplotlib then converted into
                        byte-like
                        and wrote into TFRecords. When reading the data from TF Record, instead of using the built-in
                        decode
                        image function, we use tf.io.parse_tensor following with reshaping.
                    </p>
                    <h3>Sample Prediction</h3>
                    <a data-fslightbox="gallery" href="assets/amazon/5.png"><img src="assets/amazon/5.png"></a>
                    <p>Full code of this project can be visited at <a
                            href="https://colab.research.google.com/drive/1s8iFtj7D4D0BNlsR7P9hvfzsqV8XhjTD?authuser=1"
                            class="link-out" target="blank_">Google Colaboratory</a>👌</p>
                </div>
            </section>

            <!-- <section id="further">
                <h2>
                    <a href="before2020.html" style="text-decoration:none!important; color:rgb(148, 58, 233)">Before
                        2020 →</a>
                </h2>
            </section> -->
        </div>
    </div>
</body>

<script src="assets/portrait/sketch.js"></script>
<script src="fslightbox.js"></script>

<script>

    jQuery(document).ready(function ($) {
        $(".clickable-row").click(function () {
            window.open($(this).data("href"), '_blank');
        });
    });

    // document.body.onscroll = function(){
    // document.body.style.backgroundPositionX = window.pageXOffset +"px";
    // };

</script>

</html>