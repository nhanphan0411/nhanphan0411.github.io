<!DOCTYPE html>
<html lang="en">

<head>
    <title>Nhan Phan</title>
    <meta charset="utf-8">
    <meta name="description" content="Nhan Phan">
    <meta name="author" content="Nhan Phan">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="refresh" content="600">
    <link rel="stylesheet" type="text/css" href="style.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css"
        integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.7.1/jquery.min.js"></script>


</head>

<body>
    <div class="back-to-left">
        <button onclick="topFunction()" id="myBtn" title="Go to top">Top</button>
    </div>

    <div class="page-container">
        <div class="container-offset">
            <section id="bio">
                <div class="nav-header">
                    <h1>Nh√¢n Phan</h1>
                </div>
                <div>
                    <p>
                        Nhan Phan is an artist and an educator based in Ho Chi Minh City, Vietnam.
                    </p>

                    <p>
                        Born and raised during the era of Yahoo! 360 Blog, Flickr, and Tumblr, Nhan grew to use
                        photography as his creative platform by making journals, scrapbooks, and zine ever since
                        his young age. He views photography as a documentation of time gone by, which holds not
                        just the visual but also the essence of the world he lives in. The obsession of
                        archiving images later expanded with his training in machine learning. Through analysis,
                        feature learning, and generative algorithms, Nhan's latest work explores how the realm
                        of bygone image archives influences the way we revisit our past, understand our present,
                        and pivot our future.
                    </p>

                    <p>
                        <i><a href="">Why does this page have a beach background?</a></i>
                    </p>
                </div>
            </section>

            <section id="navigation">
                <div class="nav">
                    <div class="nav-content-block">
                        <h2 class="nav-sub-header link-out"><a href="codesurfing.netlify.app" target="blank_" style="text-decoration:none!important; color:rgb(148, 58, 233)">Teach</a>
                        </h2>
                    </div>

                    <div class="nav-content-block">
                        <h2 class="nav-sub-header">Selected Works</h2>
                        <div class="nav-content pl-4 pb-3">
                            <p class="mb-1"><a href="#beach-pocket">(2022) beach pocket</a></p>
                            <p class="mb-1"><a href="#gai-gia">(2022) „Ç¨„Ç§„Ç∏„É£Âà•Â∫ú</a></p>
                            <p class="mb-1"><a href="#live-laugh-dick">(2022) live. laugh. dick(s).</a></p>
                            <p class="mb-1"><a href="#wes-anderson">(2021) wes anderson</a></p>
                            <p class="mb-1"><a href="#esrgan">(2020) esrgan</a></p>
                            <p class="mb-1"><a href="#viet-ocr">(2020) viet ocr</a></p>
                            <p class="mb-1"><a href="#amazon">(2020) amazon</a></p>
                        </div>
                    </div>

                    <div class="nav-content-block">
                        <h2 class="nav-sub-header">Before 2020</h2>
                    </div>

                    <div class="nav-content-block">
                        <h2 class="nav-sub-header">Contact</h2>
                        <div class="nav-content pl-4">
                            <p class="mb-1">nhaninsummer[at]gmail[dot]com</p>
                            <p class="mb-1"><a href="https://instagram.com/nhaninsummer" target="blank_"
                                    class="link-out">IG</a></p>
                        </div>
                    </div>

                </div>
            </section>

            <section id="beach-pocket">
                <h2>
                    (2022) Beach Pocket
                </h2>
                <div>
                    <img src="assets/beach-pocket/4.jpeg">
                    <p>
                        First thing I said when a guy took my shirt off was always ‚éØ ‚ÄúAm I too skinny?‚Äù
                    </p>
                    <p>
                        From 2019 to 2022, my best friends and I made occasional getaways to the beaches in Vietnam.
                        There we swam, sunbathed, read, danced, and radiated under the sun. There I found comfort in my
                        own skin. There I realized the beauty of our shapes. The whole process is a healing journey for
                        me.
                    </p>
                    <p>
                        This pocket notebook includes all the portraits and self-portraits that I documented during the
                        time.
                    </p>
                    <img src="assets/beach-pocket/1.jpeg">
                    <img src="assets/beach-pocket/5.jpeg">
                    <img src="assets/beach-pocket/2.jpeg">
                    <img src="assets/beach-pocket/6.jpeg">
                    <img src="assets/beach-pocket/7.jpeg">
                    <img src="assets/beach-pocket/8.jpeg">
                    <img src="assets/beach-pocket/3.jpeg">

                    <p>
                        ‚ÄúMy boyfriend once said that I was so tiny
                        That he could carry me in his pocket anywhere
                        So put me in your pocket
                        Use me as your time goes by
                        Use my body as your late-night canvas

                        Write on me
                        Compose on me
                        Fast on me
                        Slow on me
                        Release on me
                        Spit on me
                        Piss on me
                        Bleed on me‚Äù


                        ‚ú£Ôºä‚ú£

                        Produced by wedogood.
                        64 pages on risograph using aqua, yellow, flourescent pink.
                    </p>
                </div>
            </section>

            <section id="gai-gia">
                <h2>
                    (2022) „Ç¨„Ç§„Ç∏„É£Âà•Â∫ú
                </h2>
                <div>
                    <p>
                        Every year when the cicadas start to sing, I miss Japan dearly, as if a part of myself had been
                        buried under the Minami Ishigaki park, where we hung out by the swings, singing, and smoking.
                    </p>

                    <p>
                        This summer, as the cicadas are singing again, I invited Cao Mieu to join me in a conversation
                        about our Japanese memoirs. But instead of texts, we would reply to each other with artworks.
                        Every page is a response to the previous. All communication takes place only within these pages.
                    </p>

                    <p>
                        I lost my residence card years ago. Mieu still has hers, so she will hereby board the page
                        first.
                    </p>

                    <img src="assets/gai-gia/gaigiabeppu_zine01.jpg">
                    <img src="assets/gai-gia/gaigiabeppu_zine02.jpg">
                    <img src="assets/gai-gia/gaigiabeppu_zine03.jpg">
                    <img src="assets/gai-gia/gaigiabeppu_zine04.jpg">
                    <img src="assets/gai-gia/gaigiabeppu_zine05.jpg">
                    <img src="assets/gai-gia/gaigiabeppu_zine06.jpg">
                    <img src="assets/gai-gia/gaigiabeppu_zine07.jpg">
                    <img src="assets/gai-gia/gaigiabeppu_zine08.jpg">
                    <img src="assets/gai-gia/gaigiabeppu_zine09.jpg">
                    <img src="assets/gai-gia/gaigiabeppu_zine10.jpg">
                    <img src="assets/gai-gia/gaigiabeppu_zine11.jpg">
                    <img src="assets/gai-gia/gaigiabeppu_zine12.jpg">

                </div>
            </section>

            <section id="live-laugh-dick">
                <h2>
                    (2022) Live. Laugh. Dick(s).
                </h2>
                <div>
                    <img src="assets/live-laugh-dicks/1.png">
                    <img src="assets/live-laugh-dicks/2.png">
                    <p>
                        Back in April, wedogood invited me to join their zine with the theme of ‚ÄúLove Machine. Machine
                        Love‚Äù. And all I brought was erotism, fantasy, and re-imagination. This poster is a stand-alone
                        version of my work in the zine. More than a collection of quirky-looking toys, It reflects our
                        current perception of sex toy design (dildos and butt plugs in particular) while suggesting new
                        boundaries for toy design.
                    </p>

                    <img src="assets/live-laugh-dicks/3.png">
                    <img src="assets/live-laugh-dicks/4.png">

                    <p>
                        After being trained with 3000 photos of toys, the generative model clearly gets the idea that a
                        sex toy needs to be pointed (of course). But it takes the idea further by re-imagining toys with
                        multiple heads, and toys with irregular shapes or shapes that are different from cylinders.
                        Several generated samples also include toys that are bound together since e-commerce often
                        places their toys next to each other in product photos. If such an arrangement stimulates the
                        buyer, then why not include them in the real product design? Many of the generated samples also
                        propose getting rid of the inside of the toys as it is not a significant feature. They suggest
                        void, disjoint parts, transparent material, and anything else but the common solid shape.

                        Pleasure has its own curiosity. And maybe toys for pleasure should also be more suggestive,
                        rather than adaptive.

                    </p>
                    <p>

                        This project is built on my custom GAN model, inspired by StyleGAN2. The StyleGAN2 architecture
                        itself is gigantic. To afford training, I made multiple adjustments in the architecture,
                        including downsizing the output image size to 128x128. This seriously damaged the print quality
                        but Risograph helped me bypass that. I also divided the training into multiple sessions + used
                        the Tensorflow Data Dataset & Tensorflow Record to optimize the whole training speed. All are
                        for this project to be run on the free resource of Google Colab, which has a limited quota every
                        day. So much engineering just to have more dicks while paying less ü•¥

                    </p>

                    <img src="assets/live-laugh-dicks/5.png">
                    <img src="assets/live-laugh-dicks/6.gif">
                    <img src="assets/live-laugh-dicks/7.png">
                    <img src="assets/live-laugh-dicks/8.gif">

                    <p>
                        WHY RISOGRAPH?

                        Generative art is not for size queen. Artworks generated from ML model struggle to have a good
                        resolution. A simple image of 300x300 would take 90,000 units when being flattened. It means
                        that a larger output images come with a larger cost of computation. It often requires days of
                        training on expensive GPU. When it comes to printing, this limit in output results in pixelating
                        details, blurry edges, and inconsistent separation between object and background. Not only that,
                        generative images oftentimes have the checkerboard effect, as a result that the machine
                        ‚Äúpainted‚Äù each pixel independently and lack of perception of the image as a whole.

                    </p>
                    <p>
                        In order to produce this digital artwork in high-quality print (A3), we first put the 128x128
                        generated images through a half-toned treatment - a technique to simulate the image tone through
                        dots. By carefully adjusting the dot size, we gave the pixelated images a sharper optic illusion
                        in general. A subtly similar pair of aqua ink and purple paper were then chosen to let the
                        half-toned dots blend smoothly with the background. The various size of dots + different % of
                        ink embrace the blurry edge. The aqua ink also expands optically when we tilt the poster to
                        different light direction. Object edges ‚Äúfade‚Äù gradually into paper like chalk. The drawback of
                        pixelating and not having sharp edges is now a compliment toward the initial inspiration of
                        stains.
                    </p>
                    <img src="assets/live-laugh-dicks/9.png">
                    <img src="assets/live-laugh-dicks/10.png">

                    <p>
                        Crossing between multiple forms ‚éØ from photographs, to numbers, to logic, to a new form of
                        photographs, then comes alive as a print. A journey from modern computation to a long-lived
                        printing technique; from abstract to physical, with which we can see, can touch, and can
                        interact.

                        I think it is beautiful.
                    </p>
                    <img src="assets/live-laugh-dicks/11.png">
                    <img src="assets/live-laugh-dicks/12.png">


                </div>
            </section>

            <section id="wes-anderson">
                <div>
                    <h2>
                        (2021) Watching Wes Anderson Without Watching Wes Anderson
                    </h2>
                    <img src="assets/wes-anderson/aqua_1.png">
                    <img src="assets/wes-anderson/dog_1.png">
                    <img src="assets/wes-anderson/hotel_1.png">
                    <img src="assets/wes-anderson/moon_1.png">

                    <p class="pt-2">
                        In 2021, Saigon went dormant under COVID lockdown. No one can set foot beyond their door. So did
                        I
                        and my housemate. We ended up doing a marathon of Wes Anderson movies. Going through all his
                        movies
                        was like walking on a train, that moving so fast, that all the beautiful scenes become color
                        running
                        across my window. Color is the main actor in his movies.
                    </p>
                    <p>
                        The normal way to watch a movie requires audiences to sit through frame by frame. A movie
                        presents
                        itself linearly with time with visual elements built up on top of each other. This project
                        challenges that concept and aims to understand the visual landscape of Wes Anderson movies
                        through
                        just one single look.
                    </p>
                    <p>
                        To achieve that, each frame of the film was flattened from a rectangular shape (720x1280) to
                        into a
                        long strip (1x921,600). Then, all the strips were stacked on top of each other to create the
                        final
                        artwork. As a result, vertically, from top to bottom, we are ‚Äúwatching‚Äù the movie from the
                        beginning
                        to the end. Horizontally, from left to right, our eyes are moving zig-zag in one scene of the
                        movie
                        (left to right, top to bottom).
                    </p>
                    <p>
                        In the end, this project reveals how Wes Anderson uses colors to create the world surrounding
                        his
                        characters, and how that colorful world flows according to his characters' emotion.
                    </p>
                </div>
            </section>

            <section id="esrgan">
                <h2>
                    (2020) Enhanced Super Resolution GAN on Tensorflow 2
                </h2>
                <div>
                    <img src="assets/esrgan/1.jpeg">
                    <p>
                        VISION2020 aims at recovering a high resolution image from a low resolution one. The project is
                        based largely on the excellent research of Xintao Wang, et al. on ESRGAN (2018) and their
                        implementation using Pytorch. Inspired from the research, my version of ESRGAN is optimized and
                        built entirely on Tensorflow 2.0. It successfully resizes the image up to x64 on square area.
                    </p>
                    <p>
                        Single image super-resolution (SISR), as a fundamental low-level vision problem, has attracted
                        increasing attention in the research community and AI companies. SISR aims at recovering a
                        high-resolution (HR) image from a single low-resolution (LR) one. Since the pioneer work of
                        SRCNN
                        proposed by Dong et al., deep convolution neural network (CNN) approaches have brought
                        prosperous
                        development. Various network architecture designs and training strategies have continuously
                        improved
                        the SR performance.
                    </p>
                    <p>

                        The Super-Resolution Generative Adversarial Network (SRGAN) is a seminal work that is capable of
                        generating realistic textures during single image super-resolution. However, the hallucinated
                        details are often accompanied with unpleasant artifacts. To further enhance the visual quality,
                        we
                        thoroughly study three key components of SRGAN - network architecture, adversarial loss and
                        perceptual loss, and improve each of them to derive an Enhanced SRGAN (ESRGAN).
                    </p>
                    <p>
                        In particular, we introduce the Residual-in-Residual Dense Block (RRDB) without batch
                        normalization
                        as the basic network building unit. Moreover, we borrow the idea from relativistic GAN to let
                        the
                        discriminator predict relative realness instead of the absolute value. Finally, we improve the
                        perceptual loss by using the features before activation, which could provide stronger
                        supervision
                        for brightness consistency and texture recovery. Benefiting from these improvements, the
                        proposed
                        ESRGAN achieves consistently better visual quality with more realistic and natural textures than
                        SRGAN.
                    </p>

                    <img src="assets/esrgan/2.png">
                    <p>
                        fig1 ‚éØ (x4 per dimension) generated image successfully retains small detail like the strip at
                        the shoulder area and the human head.
                    </p>

                    <img src="assets/esrgan/3.png">
                    <p>
                        fig2 ‚éØ (x4 per dimension) Natural features like eyes are well reconstructed.
                    </p>

                    <img src="assets/esrgan/4.png">
                    <p>
                        fig3 ‚éØ (x8 per dimension) Double challenging, then model successfully reconstruct pattern and
                        lines.
                    </p>

                    <img src="assets/esrgan/5.png">
                    <p>
                        fig4 ‚éØ (x8 dimension) Letters are brought back to vision.
                    </p>

                    <p>
                        Full project code is available on <a href="https://github.com/nhanphan0411/VISION2020"
                            class="link-out" target="blank_">Github</a>
                    </p>

                </div>
            </section>

            <section id="viet-ocr">
                <h2>
                    (2020) Vietnamese Handwritten Optical Character Recogition
                </h2>
                <div>
                    <p>
                        Optical Character Recognition is one active field that bridges between computer vision and
                        natural language processing. As much as the field emerges within machine learning community, it
                        still performs poorly on local language, including Vietnamese with our distinctive symbol (·ªÖ, ·∫©,
                        ·ª© for example). The lack of data is one of the main reason behind it. In 2018, Cinnamon AI aimed
                        to solve that challange by hosting a hackathon with a Vietnamese handwritten dataset. It
                        includes all the address written in Vietnamese. The model can be immediately apply in post
                        service to alleviate the need of manual input.
                    </p>
                    <p>
                        All code of this project can be found on my <a href="https://github.com/nhanphan0411/viet-ocr"
                            class="link-out" target="blank_">Github üëæ</a>
                    </p>

                    <img src="assets/ocr/1.png">

                    <p>
                        ‚ùä RESULT ‚ùä

                        My project successfully achieved
                        Character Error Rate: 0.04
                        Word Error Rate: 0.14
                        Sentence Error Rate: 0.82
                    </p>
                    <p>
                        The hackathon's winner score is 0.1x on the Word Error Rate.
                        Other metric results were not disclosed.
                    </p>
                    <p>
                        ‚ùä SAMPLE PREDICTIONS ‚ùä
                        T = True Label
                        P = Prediction
                    </p>
                    <img src="assets/ocr/2.png">
                    <p>
                        ‚ùä IMAGE PREPROCESS ‚ùä
                        The preprocess was built mainly on OpenCV with 3 phases
                        1/ Thresholding
                        2/ Resize to 128x1024
                        3/ Remove Recursive (reference to A. Vinciarelli and J. Luettin)

                        (Before - After)
                    </p>
                    <img src="assets/ocr/3.png">
                    <img src="assets/ocr/4.png">
                    <p>
                        ‚ùä MODEL ‚ùä
                        CRNN + CTC Loss is used to solve this challenge.
                        CNN blocks with skip connections (inspired by ResNet50) are used to extract the features from
                        the input image.
                        The extracted feature map will be then passed through the LSTM layers.
                    </p>
                    <img src="assets/ocr/5.png">

                    <p>Training Log</p>
                    <img src="assets/ocr/6.png">



                </div>
            </section>

            <section id="amazon">
                <h2>
                    (2020) Understand The Amazon From Above
                </h2>
                <div>
                    <img src="assets/amazon/1.png">
                    <p>
                        Every minute, the world loses an area of forest the size of 48 football fields. And
                        deforestation in the Amazon Basin accounts for the largest share, contributing to reduced
                        biodiversity, habitat loss, climate change, and other devastating effects. But better data about
                        the location of deforestation and human encroachment on forests can help governments and local
                        stakeholders respond more quickly and effectively.
                    </p>

                    <p>
                        This analysis uses Deep Learning to classify the spatial images of the Amazon forest taken by
                        the satellite. From that, it hopes to shed a light on understanding how the forest has change
                        naturally and manually. Thus, help preventing deforestation.
                    </p>

                    <img src="assets/amazon/2.png">

                    <p>
                        The project is built on dataset from the Kaggle competition in 2016. It contains more than
                        40.000 images, taken by Planet using sattelites.
                    </p>
                    <p>
                        Planet, designer and builder of the world‚Äôs largest constellation of Earth-imaging satellites,
                        will soon be collecting daily imagery of the entire land surface of the earth at 3-5 meter
                        resolution. While considerable research has been devoted to tracking changes in forests, it
                        typically depends on coarse-resolution imagery from Landsat (30 meter pixels) or MODIS (250
                        meter pixels). This limits its effectiveness in areas where small-scale deforestation or forest
                        degradation dominate.
                    </p>

                    <p>
                        ‚òÜ RESULT
                        The project successfully got the score of 0.90 on the official test set.
                    </p>
                    <img src="assets/amazon/3.png">
                    <img src="assets/amazon/4.png">

                    <p>‚òÜ CHALLENGE</p>
                    <p>1Ô∏è‚É£ Multi-label: Each image is labeled with multiple tags (at least 2, at max 9). The tags fall
                        into
                        17 categories, which are the forest landscape types. Since the tags in each label are mutually
                        exclusive, they are treated as multiple binary classification problems. Thus, binary
                        cross-entropy
                        are chosen to be the loss function.
                    </p>

                    <p>
                        2Ô∏è‚É£ Imbalance: The dataset is severely imbalance with tags like Primary or Agriculture appear in
                        90%
                        of the dataset. While other tags like Blooming or Conventional Mine can only be seen in less
                        than
                        500 observations (even less than 100 for Burn Down).
                    </p>
                    <p>
                        In the first base-line experiment, the model was totally bias toward the major tags. It predicts
                        the
                        major tags to appear in every data and almost never made a prediction with the minor tags.
                        To tackle the problem of imbalance dataset, evaluation metrics must be chosen carefully. F2 is
                        chosen to be the main metrics to evaluate the training. It watches over the harmonic mean
                        between
                        the Precision and Recall while favors Recall specifically. In other word, it is the attempt to
                        reduce the number of False Negative, where the model fails to identify the absence of a tag.
                    </p>
                    <p>
                        3Ô∏è‚É£ Optimization: 400.000 images, a CNN model, and Google Colab's limited resource do not seem
                        to
                        mix well together. The training was slow at first and interupted often. Several improvements,
                        mostly
                        on the Tensorflow pipeline, were conducted to speed up the training:
                        Using TFRecord to convert the raw images into byte-like data to reduce the amount of time
                        spending
                        on reading data from their paths.
                    </p>
                    <p>
                        Using tf.data.Dataset with shuffle, map, batch, prefetch to optimize the reading data process by
                        redistributing the tasks for agents to work concurrently, thus, avoid bottleneck. An attempt to
                        use
                        cache was also made but failed due to the limited RAM.
                    </p>
                    <p>
                        Processing image with Tensorflow: The dataset contains images in JPG - RGBA. The built-in decode
                        function tf.io.decode_jpeg only works on 1 or 3-channel image. Attempt on encoding a JPG RGBA
                        image
                        returns black black and black. We need a tensorflow encoding function to work in this part
                        because
                        the pipeline is built entirely on Tensor for the optimization purpose.
                        To tackle the problem, the raw images were first read by Matplotlib then converted into
                        byte-like
                        and wrote into TFRecords. When reading the data from TF Record, instead of using the built-in
                        decode
                        image function, we use tf.io.parse_tensor following with reshaping.
                    </p>
                    <p>‚òÜ SAMPLE PREDICTIONS</p>
                    <img src="assets/amazon/5.png">
                    <p>Full code of this project can be visited at <a
                            href="https://colab.research.google.com/drive/1s8iFtj7D4D0BNlsR7P9hvfzsqV8XhjTD?authuser=1"
                            class="link-out" target="blank_">Google Colaboratory</a>üëå</p>
                </div>
            </section>

            <section id="further">
                <h2>
                    <a href="before2020.html">Before 2020 ‚Üí</a>
                </h2>
            </section>
        </div>
    </div>
</body>

<script>
    jQuery(document).ready(function ($) {
        $(".clickable-row").click(function () {
            window.open($(this).data("href"), '_blank');
        });
    });

</script>

</html>